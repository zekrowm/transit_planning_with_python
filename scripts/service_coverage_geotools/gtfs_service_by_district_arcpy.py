"""Generates a matrix showing which public transit routes operate near which districts.

This script processes General Transit Feed Specification (GTFS) data and performs a
spatial analysis using ArcPy to determine the relationship between transit stops
and specified districts.

It follows these steps:
1. Loads required GTFS files (routes, trips, stops, stop_times).
2. Filters GTFS stops to include only active boarding locations.
3. Converts the filtered stops to a point feature class in WGS84 (EPSG:4326).
4. Projects both the stops and the districts feature class (shapefile) to a
   local projected coordinate system (TARGET_EPSG).
5. Performs a spatial join (WITHIN_A_DISTANCE, defined by SEARCH_DISTANCE_FEET)
   to link GTFS stops to districts.
6. Maps routes to districts if any stop on the route is spatially joined to that district.
7. Outputs the final route-district matrix as an Excel file.
"""

from __future__ import annotations

import logging
import os
import sys
import uuid
from collections.abc import Mapping, Sequence
from typing import Any, Optional

import arcpy
import pandas as pd

# =============================================================================
# CONFIGURATION
# =============================================================================

DISTRICTS_FC = r"Path\To\Your\Districts.shp"
GTFS_DIR = r"Path\To\Your\GTFS_data"
GTFS_FILES = ["routes.txt", "stops.txt", "trips.txt", "stop_times.txt"]

TARGET_EPSG = 2248
SEARCH_DISTANCE_FEET = 1320.0
DISTRICT_FIELD = "DISTRICT"

# FAST: local SSD for intermediates
WORK_DIR = r"C:\temp\gtfs_district_matrix_work"
WORK_GDB_NAME = "work.gdb"

OUTPUT_EXCEL = r"Path\To\Your\Excel_File.xlsx"
LOG_DIR = r"Path\To\Your\Logs"

# =============================================================================
# LOGGING
# =============================================================================


def configure_logging(log_dir: str) -> None:
    """Sets up root logging to output to both stdout and a file in the specified directory.

    Args:
        log_dir: The directory where the log file should be created.
    """
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, "gtfs_route_district_matrix.log")

    root = logging.getLogger()
    root.setLevel(logging.INFO)
    for h in list(root.handlers):
        root.removeHandler(h)

    fmt = logging.Formatter("%(asctime)s — %(levelname)s — %(message)s")

    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    root.addHandler(sh)

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(fmt)
    root.addHandler(fh)

    logging.info("Logging to: %s", log_path)


def log_arcpy_messages(level: int = logging.INFO) -> None:
    """Logs any messages generated by the last ArcPy operation."""
    msg = arcpy.GetMessages()
    if msg:
        logging.log(level, "ArcPy messages:\n%s", msg)


# =============================================================================
# UTILITIES
# =============================================================================


def safe_name(prefix: str, workspace: str) -> str:
    """Generates a unique and valid name for an ArcGIS feature class or table.

    Args:
        prefix: The base prefix for the name (e.g., 'stops_wgs84').
        workspace: The workspace (GDB) where the name will be used.

    Returns:
        A validated, unique name string.
    """
    suffix = uuid.uuid4().hex[:8]
    return arcpy.ValidateTableName(f"{prefix}_{suffix}", workspace)


def ensure_work_gdb(work_dir: str, gdb_name: str) -> str:
    """Creates a file geodatabase (GDB) if it doesn't already exist.

    Args:
        work_dir: The directory where the GDB should reside.
        gdb_name: The name of the GDB file (e.g., 'work.gdb').

    Returns:
        The full path to the geodatabase.
    """
    os.makedirs(work_dir, exist_ok=True)
    gdb = os.path.join(work_dir, gdb_name)
    if not arcpy.Exists(gdb):
        logging.info("Creating work GDB: %s", gdb)
        arcpy.management.CreateFileGDB(work_dir, gdb_name)
        log_arcpy_messages()
    return gdb


# =============================================================================
# GTFS LOADING + FILTERING
# =============================================================================


def load_gtfs_data(
    gtfs_folder_path: str,
    files: Optional[Sequence[str]] = None,
    dtype: str | type[str] | Mapping[str, Any] = str,
) -> dict[str, pd.DataFrame]:
    """Load one or more GTFS text files into memory.

    Args:
        gtfs_folder_path: Absolute or relative path to the folder
            containing the GTFS feed.
        files: Explicit sequence of file names to load. If ``None``,
            the standard 13 GTFS text files are attempted.
        dtype: Value forwarded to :pyfunc:`pandas.read_csv(dtype=…)` to
            control column dtypes. Supply a mapping for per-column dtypes.

    Returns:
        Mapping of file stem → :class:`pandas.DataFrame`; for example,
        ``data["trips"]`` holds the parsed *trips.txt* table.

    Raises:
        OSError: Folder missing or one of *files* not present.
        ValueError: Empty file or CSV parser failure.
        RuntimeError: Generic OS error while reading a file.

    Notes:
        All columns default to ``str`` to avoid pandas’ type-inference
        pitfalls (e.g. leading zeros in IDs).
    """
    if not os.path.exists(gtfs_folder_path):
        raise OSError(f"The directory '{gtfs_folder_path}' does not exist.")

    if files is None:
        files = (
            "agency.txt",
            "stops.txt",
            "routes.txt",
            "trips.txt",
            "stop_times.txt",
            "calendar.txt",
            "calendar_dates.txt",
            "fare_attributes.txt",
            "fare_rules.txt",
            "feed_info.txt",
            "frequencies.txt",
            "shapes.txt",
            "transfers.txt",
        )

    missing = [
        file_name
        for file_name in files
        if not os.path.exists(os.path.join(gtfs_folder_path, file_name))
    ]
    if missing:
        raise OSError(f"Missing GTFS files in '{gtfs_folder_path}': {', '.join(missing)}")

    data: dict[str, pd.DataFrame] = {}
    for file_name in files:
        key = file_name.replace(".txt", "")
        file_path = os.path.join(gtfs_folder_path, file_name)
        try:
            df = pd.read_csv(file_path, dtype=dtype, low_memory=False)
            data[key] = df
            logging.info("Loaded %s (%d records).", file_name, len(df))

        except pd.errors.EmptyDataError as exc:
            raise ValueError(f"File '{file_name}' in '{gtfs_folder_path}' is empty.") from exc

        except pd.errors.ParserError as exc:
            raise ValueError(
                f"Parser error in '{file_name}' in '{gtfs_folder_path}': {exc}"
            ) from exc

        except OSError as exc:
            raise RuntimeError(
                f"OS error reading file '{file_name}' in '{gtfs_folder_path}': {exc}"
            ) from exc

    return data





def filter_stops(gtfs_data: dict[str, pd.DataFrame]) -> pd.DataFrame:
    """Keep only stops that actually appear in stop_times and are boarding locations."""
    stops = gtfs_data["stops"].copy()
    stop_times = gtfs_data["stop_times"]

    used_ids = set(stop_times["stop_id"].astype(str).unique())
    stops["stop_id"] = stops["stop_id"].astype(str)
    stops = stops[stops["stop_id"].isin(used_ids)]

    if "location_type" in stops.columns:
        stops = stops[stops["location_type"].fillna("0").astype(str) == "0"]

    stops = stops[["stop_id", "stop_lon", "stop_lat"]].dropna()
    stops["stop_lon"] = stops["stop_lon"].astype(float)
    stops["stop_lat"] = stops["stop_lat"].astype(float)

    logging.info("Filtered to %s active boarding stops", len(stops))
    return stops


# =============================================================================
# SPATIAL WORK
# =============================================================================


def csv_to_points(
    csv_path: str,
    out_gdb: str,
) -> str:
    """Converts a CSV table of latitude/longitude coordinates to a point feature class.

    The output feature class is created in the WGS84 (EPSG:4326) coordinate system.

    Args:
        csv_path: Path to the input CSV file (must contain stop_lon and stop_lat).
        out_gdb: Path to the geodatabase for the output feature class.

    Returns:
        The full path to the created point feature class.
    """
    out_fc = os.path.join(out_gdb, safe_name("stops_wgs84", out_gdb))
    logging.info("XYTableToPoint from CSV")
    arcpy.management.XYTableToPoint(
        in_table=csv_path,
        out_feature_class=out_fc,
        x_field="stop_lon",
        y_field="stop_lat",
        coordinate_system=arcpy.SpatialReference(4326),
    )
    log_arcpy_messages()
    return out_fc


def project_fc(in_fc: str, out_gdb: str, epsg: int, prefix: str) -> str:
    """Projects an input feature class to a new coordinate system defined by an EPSG code.

    Args:
        in_fc: The path to the input feature class.
        out_gdb: The output geodatabase.
        epsg: The EPSG code of the target coordinate system.
        prefix: The prefix to use for naming the output feature class.

    Returns:
        The full path to the projected feature class.
    """
    out_fc = os.path.join(out_gdb, safe_name(prefix, out_gdb))
    logging.info("Projecting %s → EPSG:%s", os.path.basename(in_fc), epsg)
    arcpy.management.Project(in_fc, out_fc, arcpy.SpatialReference(epsg))
    log_arcpy_messages()
    return out_fc


def spatial_join_stops_to_districts(
    stops_fc: str,
    districts_fc: str,
    district_field: str,
    search_dist_ft: float,
    out_gdb: str,
) -> str:
    """Performs a spatial join to link stops to nearby districts.

    Args:
        stops_fc: Projected transit stops feature class.
        districts_fc: Projected districts feature class.
        district_field: The field in the districts feature class to carry over.
        search_dist_ft: The maximum distance (in feet) to join the stop to a district.
        out_gdb: The output geodatabase.

    Returns:
        The full path to the spatially joined output feature class.
    """
    out_fc = os.path.join(out_gdb, safe_name("stops_districts", out_gdb))
    logging.info("SpatialJoin WITHIN_A_DISTANCE (%s ft)", search_dist_ft)

    arcpy.analysis.SpatialJoin(
        target_features=stops_fc,
        join_features=districts_fc,
        out_feature_class=out_fc,
        join_operation="JOIN_ONE_TO_MANY",
        join_type="KEEP_COMMON",
        match_option="WITHIN_A_DISTANCE",
        search_radius=f"{search_dist_ft} Feet",
    )
    log_arcpy_messages()
    return out_fc


def extract_stop_districts(fc: str, district_field: str) -> dict[str, set[str]]:
    """Reads the spatial join result and creates a mapping of stop_id to a set of districts.

    Args:
        fc: The spatially joined feature class.
        district_field: The name of the district ID field.

    Returns:
        A dictionary mapping stop ID strings to a set of district ID strings.
    """
    stop_to_districts: dict[str, set[str]] = {}
    with arcpy.da.SearchCursor(fc, ["stop_id", district_field]) as cur:
        for stop_id, dist in cur:
            if stop_id and dist:
                stop_to_districts.setdefault(str(stop_id), set()).add(str(dist))
    return stop_to_districts


# =============================================================================
# MATRIX BUILD
# =============================================================================


def build_route_district_matrix(
    gtfs_data: dict[str, pd.DataFrame],
    stop_to_districts: dict[str, set[str]],
) -> pd.DataFrame:
    """Builds the final route-district matrix DataFrame.

    A route is linked to a district if any stop on that route is near that district.

    Args:
        gtfs_data: Dictionary containing GTFS DataFrames (routes, trips, stop_times).
        stop_to_districts: Mapping of stop ID to the set of nearby district IDs.

    Returns:
        A pandas DataFrame representing the route-district coverage matrix.
    """
    routes = gtfs_data["routes"]
    trips = gtfs_data["trips"]
    stop_times = gtfs_data["stop_times"]

    route_name = dict(
        zip(
            routes["route_id"].astype(str),
            routes["route_short_name"].astype(str),
            strict=True,
        )
    )
    trip_route = dict(
        zip(
            trips["trip_id"].astype(str),
            trips["route_id"].astype(str),
            strict=True,
        )
    )

    stop_routes: dict[str, set[str]] = {}
    for t_id, s_id in stop_times[["trip_id", "stop_id"]].itertuples(index=False):
        rid = trip_route.get(str(t_id))
        if rid:
            stop_routes.setdefault(str(s_id), set()).add(rid)

    route_districts: dict[str, set[str]] = {}
    for stop_id, districts in stop_to_districts.items():
        for rid in stop_routes.get(stop_id, []):
            route_districts.setdefault(rid, set()).update(districts)

    all_routes = sorted(route_districts, key=lambda r: route_name.get(r, "zzz"))
    all_districts = sorted({d for ds in route_districts.values() for d in ds})

    rows: list[dict[str, str]] = []
    for rid in all_routes:
        row: dict[str, str] = {"route_short_name": route_name.get(rid, rid)}
        for d in all_districts:
            row[d] = "y" if d in route_districts[rid] else "n"
        rows.append(row)

    return pd.DataFrame(rows)


# =============================================================================
# MAIN
# =============================================================================


def main() -> None:
    """Main execution function to run the GTFS-district spatial analysis workflow."""
    configure_logging(LOG_DIR)
    arcpy.env.overwriteOutput = True

    work_gdb = ensure_work_gdb(WORK_DIR, WORK_GDB_NAME)
    arcpy.env.workspace = work_gdb
    logging.info("Workspace: %s", work_gdb)

    gtfs = load_gtfs_data(GTFS_DIR, GTFS_FILES)
    stops_df = filter_stops(gtfs)

    csv_path = os.path.join(WORK_DIR, "filtered_stops.csv")
    stops_df.to_csv(csv_path, index=False)

    stops_wgs84 = csv_to_points(csv_path, work_gdb)
    stops_proj = project_fc(stops_wgs84, work_gdb, TARGET_EPSG, "stops_proj")
    districts_proj = project_fc(DISTRICTS_FC, work_gdb, TARGET_EPSG, "districts_proj")

    sj_fc = spatial_join_stops_to_districts(
        stops_proj,
        districts_proj,
        DISTRICT_FIELD,
        SEARCH_DISTANCE_FEET,
        work_gdb,
    )

    stop_to_districts = extract_stop_districts(sj_fc, DISTRICT_FIELD)
    df = build_route_district_matrix(gtfs, stop_to_districts)

    os.makedirs(os.path.dirname(OUTPUT_EXCEL), exist_ok=True)
    df.to_excel(OUTPUT_EXCEL, index=False)
    logging.info("Done. Excel written to: %s", OUTPUT_EXCEL)


if __name__ == "__main__":
    main()
